[
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Building Models",
    "section": "",
    "text": "Before we begin building models, we will discuss the nature of this site and what its intentions are\n\n\nThe data set we will be using throughout the entirety of our project will be the Diabetes Binary Health Indicators data set. This is a clean data set of 253,680 survey responses conducted by the CDC, in which the target variable is Diabetes_binary - where 0 denotes no diabetes, and 1 denotes pre-diabetes or diabetes. Also included in this data set, are 21 feature variables, that are not balanced, in which can hopefully be used to explain whether or not a person will have diabetes.\nWe will only be interested in exploring 14 out of the 21 feature variables, and the next section will dive deeper into those that we will use.\n\n\nBelow, is a list of the important variables I have identified that should be beneficial to explore in relation to our response variable of interest - Diabetes_binary (explained above):\n\nHighBP : Do you have high blood pressure? 0 = no; 1 = yes (categorical)\nHighChol : Do you have high cholesterol? 0 = no; 1 = yes (categorical)\nCholCheck : Have you had your cholesterol checked in past 5 years? 0 = no; 1 = yes (categorical)\nBMI : Body Mass Index value (numeric)\nSmoker : Have you smoked at least 100 cigarette’s in their life? 0 = no; 1 = yes (categorical)\nStroke : Have you ever been told you had a stroke? 0 = no; 1 = yes (categorical)\nHeartDiseaseorAttack : Have you ever had coronary heart disease (CHD) or myocardial infarction (MI)? 0 = no; 1 = yes (categorical)\nPhysActivity : Not including you job, ave you done any physical activity in past 30 days? 0 = no; 1 = yes (categorical)\nHvyAlchoholConsump : Do you consume at least 14 (men) or at least 7 (women) drinks per week? 0 = no; 1 = yes (categorical)\nGenHlth : What would you say your general health is? 1 = excellent; 2 = very good; 3 = good; 4 = fair; 5 = poor (categorical/ordinal)\nPhysHlth : Denotes how many days over the past 30 days you felt your physical health was bad (illness/injury)\nDiffWalk : Do you have serious difficulty walking or climbing stairs? 0 = no; 1 = yes (categorical)\nSex : What is your sex? 0 = female; 1 = male (categorical)\nAge : How old are you? 13-level age category (categorical)\n\n\n\n\n\nThe purpose of this document is to apply what we have learned in the exploratory data analysis page, and build some models to try and predict whether or not an individual will have diabetes. This document will make use of three different types of models: Logistic Regression, Classification Trees, and Random Forests. The descriptions of these models will be outlined in their respective sections. To compare these models, we will use log-loss as our criteria for defining the best model.\nWe are going to be using log-loss as our criteria, instead of accuracy, as mentioned in the above paragraph. Loss-loss criteria, popularly used in binary/classification analyses, is a special form of the likelihood function, and can be used to assess performance of classification models. Unlike certain criteria (i.e., accuracy), Log-loss penalizes models more heavily when incorrectly classifying observations. This is a main reason as to why we are choosing Log-loss over accuracy.\nNOw, let us start with our model-building process!"
  },
  {
    "objectID": "Modeling.html#diabetes-health-indicators-data-set",
    "href": "Modeling.html#diabetes-health-indicators-data-set",
    "title": "Building Models",
    "section": "",
    "text": "The data set we will be using throughout the entirety of our project will be the Diabetes Binary Health Indicators data set. This is a clean data set of 253,680 survey responses conducted by the CDC, in which the target variable is Diabetes_binary - where 0 denotes no diabetes, and 1 denotes pre-diabetes or diabetes. Also included in this data set, are 21 feature variables, that are not balanced, in which can hopefully be used to explain whether or not a person will have diabetes.\nWe will only be interested in exploring 14 out of the 21 feature variables, and the next section will dive deeper into those that we will use.\n\n\nBelow, is a list of the important variables I have identified that should be beneficial to explore in relation to our response variable of interest - Diabetes_binary (explained above):\n\nHighBP : Do you have high blood pressure? 0 = no; 1 = yes (categorical)\nHighChol : Do you have high cholesterol? 0 = no; 1 = yes (categorical)\nCholCheck : Have you had your cholesterol checked in past 5 years? 0 = no; 1 = yes (categorical)\nBMI : Body Mass Index value (numeric)\nSmoker : Have you smoked at least 100 cigarette’s in their life? 0 = no; 1 = yes (categorical)\nStroke : Have you ever been told you had a stroke? 0 = no; 1 = yes (categorical)\nHeartDiseaseorAttack : Have you ever had coronary heart disease (CHD) or myocardial infarction (MI)? 0 = no; 1 = yes (categorical)\nPhysActivity : Not including you job, ave you done any physical activity in past 30 days? 0 = no; 1 = yes (categorical)\nHvyAlchoholConsump : Do you consume at least 14 (men) or at least 7 (women) drinks per week? 0 = no; 1 = yes (categorical)\nGenHlth : What would you say your general health is? 1 = excellent; 2 = very good; 3 = good; 4 = fair; 5 = poor (categorical/ordinal)\nPhysHlth : Denotes how many days over the past 30 days you felt your physical health was bad (illness/injury)\nDiffWalk : Do you have serious difficulty walking or climbing stairs? 0 = no; 1 = yes (categorical)\nSex : What is your sex? 0 = female; 1 = male (categorical)\nAge : How old are you? 13-level age category (categorical)"
  },
  {
    "objectID": "Modeling.html#purpose",
    "href": "Modeling.html#purpose",
    "title": "Building Models",
    "section": "",
    "text": "The purpose of this document is to apply what we have learned in the exploratory data analysis page"
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "Exploring Diabetes Data Set",
    "section": "",
    "text": "Before we begin exploring our data, let’s give you some information about the data set and its variables, as well as the purpose of this document and how it will relate to our modeling.\n\n\nThe data set we will be using throughout the entirety of our project will be the Diabetes Binary Health Indicators data set. This is a clean data set of 253,680 survey responses conducted by the CDC, in which the target variable is Diabetes_binary - where 0 denotes no diabetes, and 1 denotes pre-diabetes or diabetes. Also included in this data set, are 21 feature variables, that are not balanced, in which can hopefully be used to explain whether or not a person will have diabetes.\nWe will only be interested in exploring 14 out of the 21 feature variables, and the next section will dive deeper into those that we will use.\n\n\nBelow, is a list of the important variables I have identified that should be beneficial to explore in relation to our response variable of interest - Diabetes_binary (explained above):\n\nHighBP : Do you have high blood pressure? 0 = no; 1 = yes (categorical)\nHighChol : Do you have high cholesterol? 0 = no; 1 = yes (categorical)\nCholCheck : Have you had your cholesterol checked in past 5 years? 0 = no; 1 = yes (categorical)\nBMI : Body Mass Index value (numeric)\nSmoker : Have you smoked at least 100 cigarette’s in their life? 0 = no; 1 = yes (categorical)\nStroke : Have you ever been told you had a stroke? 0 = no; 1 = yes (categorical)\nHeartDiseaseorAttack : Have you ever had coronary heart disease (CHD) or myocardial infarction (MI)? 0 = no; 1 = yes (categorical)\nPhysActivity : Not including you job, ave you done any physical activity in past 30 days? 0 = no; 1 = yes (categorical)\nHvyAlchoholConsump : Do you consume at least 14 (men) or at least 7 (women) drinks per week? 0 = no; 1 = yes (categorical)\nGenHlth : What would you say your general health is? 1 = excellent; 2 = very good; 3 = good; 4 = fair; 5 = poor (categorical/ordinal)\nPhysHlth : Denotes how many days over the past 30 days you felt your physical health was bad (illness/injury)\nDiffWalk : Do you have serious difficulty walking or climbing stairs? 0 = no; 1 = yes (categorical)\nSex : What is your sex? 0 = female; 1 = male (categorical)\nAge : How old are you? 13-level age category (categorical)\n\n\n\n\n\nThe purpose of this document is to explore the variables listed above, specifically in relation to our response variable of interest Diabetes_binary. We will look at numerical/categorical summaries, as well as visualizations to see if we can highlight any important relationships or patterns in the data. This will translate to our modeling page, where we will take what we have learned from our data exploration and apply it to our model building. Here, we will identify potential influential variables and use them in our models to determine there true effect in classifying whether or not an individual has diabetes. Now, it is time to start exploring our data"
  },
  {
    "objectID": "EDA.html#diabetes-health-indicators-data-set",
    "href": "EDA.html#diabetes-health-indicators-data-set",
    "title": "Exploring Diabetes Data Set",
    "section": "",
    "text": "The data set we will be using throughout the entirety of our project will be the Diabetes Binary Health Indicators data set. This is a clean data set of 253,680 survey responses conducted by the CDC, in which the target variable is Diabetes_binary - where 0 denotes no diabetes, and 1 denotes pre-diabetes or diabetes. Also included in this data set, are 21 feature variables, that are not balanced, in which can hopefully be used to explain whether or not a person will have diabetes.\nWe will only be interested in exploring 14 out of the 21 feature variables, and the next section will dive deeper into those that we will use.\n\n\nBelow, is a list of the important variables I have identified that should be beneficial to explore in relation to our response variable of interest - Diabetes_binary (explained above):\n\nHighBP : Do you have high blood pressure? 0 = no; 1 = yes (categorical)\nHighChol : Do you have high cholesterol? 0 = no; 1 = yes (categorical)\nCholCheck : Have you had your cholesterol checked in past 5 years? 0 = no; 1 = yes (categorical)\nBMI : Body Mass Index value (numeric)\nSmoker : Have you smoked at least 100 cigarette’s in their life? 0 = no; 1 = yes (categorical)\nStroke : Have you ever been told you had a stroke? 0 = no; 1 = yes (categorical)\nHeartDiseaseorAttack : Have you ever had coronary heart disease (CHD) or myocardial infarction (MI)? 0 = no; 1 = yes (categorical)\nPhysActivity : Not including you job, ave you done any physical activity in past 30 days? 0 = no; 1 = yes (categorical)\nHvyAlchoholConsump : Do you consume at least 14 (men) or at least 7 (women) drinks per week? 0 = no; 1 = yes (categorical)\nGenHlth : What would you say your general health is? 1 = excellent; 2 = very good; 3 = good; 4 = fair; 5 = poor (categorical/ordinal)\nPhysHlth : Denotes how many days over the past 30 days you felt your physical health was bad (illness/injury)\nDiffWalk : Do you have serious difficulty walking or climbing stairs? 0 = no; 1 = yes (categorical)\nSex : What is your sex? 0 = female; 1 = male (categorical)\nAge : How old are you? 13-level age category (categorical)"
  },
  {
    "objectID": "EDA.html#purpose",
    "href": "EDA.html#purpose",
    "title": "Exploring Diabetes Data Set",
    "section": "",
    "text": "The purpose of this document is to explore the variables listed above, specifically in relation to our response variable of interest Diabetes_binary. We will look at numerical/categorical summaries, as well as visualizations to see if we can highlight any important relationships or patterns in the data. This will translate to our modeling page, where we will take what we have learned from our data exploration and apply it to our model building. Here, we will identify potential influential variables and use them in our models to determine there true effect in classifying whether or not an individual has diabetes. Now, it is time to start exploring our data"
  },
  {
    "objectID": "EDA.html#numeric-variable-exploration",
    "href": "EDA.html#numeric-variable-exploration",
    "title": "Exploring Diabetes Data Set",
    "section": "Numeric Variable Exploration",
    "text": "Numeric Variable Exploration\n\ndiabetesBMI &lt;- diabetes |&gt;\n  select(Diabetes_binary, BMI)\n\nsummary(diabetesBMI$BMI)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   24.00   27.00   28.38   31.00   98.00 \n\n\nLooking at a quick summary of the BMI variable, we can begin to see that most values will be clustered within 12-31 (75% of values); however, there will be 25% of the values that will be spread out across a range of 31 to 98. Let’s dive a bit deeper into this variable:\n\nbmiPlot &lt;- ggplot(diabetesBMI) \n\nbmiPlot + geom_histogram(aes(x = BMI), color = \"Black\", fill = \"Red\") +\n  labs(title = \"Histogram of BMI Counts\", x = \"BMI Values\") \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe histogram above, reaffirms the previous finding, and we can see we have right-skewed distribution. Let us explore its relationship with the response variable:\n\nbmiPlot +\n  geom_jitter(aes(x = BMI, y = Diabetes_binary, color = factor(Diabetes_binary))) +\n  labs(x = \"BMI Values\", y = \"Diabetes (0 = no, 1 = yes)\",\n       title = \"Scatter Plot Showing Relationship between BMI and Diabetes Status\",\n       color = \"Diabetes Status\")\n\n\n\n\nBased off of this scatter plot, there is really no glaring patterns here that I can see. Let us move to the categorical predictors to see if there is any glaring relationships."
  },
  {
    "objectID": "EDA.html#categorical-variable-exploration",
    "href": "EDA.html#categorical-variable-exploration",
    "title": "Exploring Diabetes Data Set",
    "section": "Categorical Variable Exploration",
    "text": "Categorical Variable Exploration\nIn this section, we are going to look at all of the categorical variables from our list of important variables. This will give us a better look to see which variables could be the most important in relation to the response. For a recap of variable descriptions, see the Important Variables section.\nWe are first going to look at the categorical variables with only 2-levels. Here, we will create a 2x2 contingency table from which we will turn into a data frame. We will take this data frame, group the data by the feature variable, and look at the the percentage of individuals who do/don’t have diabetes across each level of the feature variable:\n\ndiabetesCat &lt;- diabetes |&gt;\n  select(-BMI)\n\nresults &lt;- list()\n\nfor (i in names(diabetesCat)[-c(1, 10, 11, 14)]) {\n  var &lt;- i\n  contTable &lt;- table(diabetesCat$Diabetes_binary, diabetesCat[[i]])\n  contDf &lt;- as.data.frame(contTable)\n  colnames(contDf) &lt;- c(\"Diabetes\", var, \"Count\")\n  percentages &lt;- contDf |&gt;\n    group_by(!!sym(var)) |&gt;\n    mutate(Percentage = Count / sum(Count) * 100)\n  \n  results[[i]] &lt;- percentages\n}\n\nfor (i in names(results)) {\n  print(results[[i]])\n  cat(\"\\n\")\n}\n\n# A tibble: 4 × 4\n# Groups:   HighBP [2]\n  Diabetes HighBP  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 No       No     136109      94.0 \n2 Yes      No       8742       6.04\n3 No       Yes     82225      75.6 \n4 Yes      Yes     26604      24.4 \n\n# A tibble: 4 × 4\n# Groups:   HighChol [2]\n  Diabetes HighChol  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 No       No       134429      92.0 \n2 Yes      No        11660       7.98\n3 No       Yes       83905      78.0 \n4 Yes      Yes       23686      22.0 \n\n# A tibble: 4 × 4\n# Groups:   CholCheck [2]\n  Diabetes CholCheck  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;      &lt;int&gt;      &lt;dbl&gt;\n1 No       No          9229      97.5 \n2 Yes      No           241       2.54\n3 No       Yes       209105      85.6 \n4 Yes      Yes        35105      14.4 \n\n# A tibble: 4 × 4\n# Groups:   Smoker [2]\n  Diabetes Smoker  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 No       No     124228       87.9\n2 Yes      No      17029       12.1\n3 No       Yes     94106       83.7\n4 Yes      Yes     18317       16.3\n\n# A tibble: 4 × 4\n# Groups:   Stroke [2]\n  Diabetes Stroke  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 No       No     211310       86.8\n2 Yes      No      32078       13.2\n3 No       Yes      7024       68.2\n4 Yes      Yes      3268       31.8\n\n# A tibble: 4 × 4\n# Groups:   HeartDiseaseorAttack [2]\n  Diabetes HeartDiseaseorAttack  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;                 &lt;int&gt;      &lt;dbl&gt;\n1 No       No                   202319       88.0\n2 Yes      No                    27468       12.0\n3 No       Yes                   16015       67.0\n4 Yes      Yes                    7878       33.0\n\n# A tibble: 4 × 4\n# Groups:   PhysActivity [2]\n  Diabetes PhysActivity  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;         &lt;int&gt;      &lt;dbl&gt;\n1 No       No            48701       78.9\n2 Yes      No            13059       21.1\n3 No       Yes          169633       88.4\n4 Yes      Yes           22287       11.6\n\n# A tibble: 4 × 4\n# Groups:   HvyAlcoholConsump [2]\n  Diabetes HvyAlcoholConsump  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;              &lt;int&gt;      &lt;dbl&gt;\n1 No       No                204910      85.6 \n2 Yes      No                 34514      14.4 \n3 No       Yes                13424      94.2 \n4 Yes      Yes                  832       5.84\n\n# A tibble: 4 × 4\n# Groups:   DiffWalk [2]\n  Diabetes DiffWalk  Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;     &lt;int&gt;      &lt;dbl&gt;\n1 No       No       188780       89.5\n2 Yes      No        22225       10.5\n3 No       Yes       29554       69.3\n4 Yes      Yes       13121       30.7\n\n# A tibble: 4 × 4\n# Groups:   Sex [2]\n  Diabetes Sex    Count Percentage\n  &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt;      &lt;dbl&gt;\n1 No       0     123563       87.0\n2 Yes      0      18411       13.0\n3 No       1      94771       84.8\n4 Yes      1      16935       15.2\n\n\nLooking at the above data sets, we can see that there are some variables that should influence whether or not an individual has diabetes. One of the more interesting findings that we see here is in the HvyAlcoholConsump data set. Those who do not drink heavily have a larger percentage of diabetes than those who do drink heavily. There are also some findings that are inline with what we would expect. For example, 22% of individuals with high cholesterol also have diabetes, whereas only ~8% of individuals without high cholesterol have diabetes.\nNow, let’s look at the categorical variables that have more than two levels. We will first look at the age variable. Let us do the same thing we did above, but as bar plots. We will see how the percentages of people with diabetes changes across the different age groups.\n\nageTable &lt;- table(diabetesCat$Age, diabetesCat$Diabetes_binary) |&gt;\n  as.data.frame()\n\ncolnames(ageTable) &lt;- c(\"Age\", \"Diabetes\", \"Count\")\n\nageTable &lt;- ageTable |&gt;\n  group_by(Age) |&gt;\n  mutate(Percentage = Count / sum(Count) * 100) |&gt;\n  ungroup()\n\nggplot(ageTable, aes(x = Age, y = Percentage, fill = Diabetes)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(x = \"Age Category\", y = \"Percentage\",\n       title = \"Percentage of Individuals with Diabetes for each Age Group\",\n       fill = \"Diabetes\") \n\n\n\n\nFrom this graph, we can see that as we increase our age category, a larger percentage of individuals have Diabetes. Now, let us investigate the General Health variable the same way:\n\ngenHlthTable &lt;- table(diabetesCat$GenHlth, diabetesCat$Diabetes_binary) |&gt;\n  as.data.frame()\n\ncolnames(genHlthTable) &lt;- c(\"GenHlth\", \"Diabetes\", \"Count\")\n\ngenHlthTable &lt;- genHlthTable |&gt;\n  group_by(GenHlth) |&gt;\n  mutate(Percentage = Count / sum(Count) * 100) |&gt;\n  ungroup()\n\nggplot(genHlthTable, aes(x = GenHlth, y = Percentage, fill = Diabetes)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(x = \"General Health Category\", y = \"Percentage\",\n       title = \"Percentage of Individuals with Diabetes for each General Health Group\",\n       fill = \"Diabetes\") \n\n\n\n\nThis also tells us that as our general health gets worse, the percentage of people with diabetes increases. For the last plot, let us explore the Physical Health variable:\n\nphysHlthTable &lt;- table(diabetesCat$PhysHlth, diabetesCat$Diabetes_binary) |&gt;\n  as.data.frame()\n\ncolnames(physHlthTable) &lt;- c(\"PhysHlth\", \"Diabetes\", \"Count\")\n\nphysHlthTable &lt;- physHlthTable |&gt;\n  group_by(PhysHlth) |&gt;\n  mutate(Percentage = Count / sum(Count) * 100) |&gt;\n  ungroup()\n\nggplot(physHlthTable, aes(x = PhysHlth, y = Percentage, fill = Diabetes)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(x = \"Physical Health Category\", y = \"Percentage\",\n       title = \"Percentage of Individuals with Diabetes for each Physical Health Group\",\n       fill = \"Diabetes\") \n\n\n\n\nThe pattern in this plot is not as noticeable as the others, but we can still see that has the number of days where your physical health was not good, the percentages of having diabetes increase.\n#Conclusion\nThrough this exploratory data analysis of our data, we have investigated several variables’ relationship to the response of interest - Diabetes_binary. Through numerical summaries and scatter plots, contingency tables and bar plots, there have been many findings. Some of the findings that we had were surprising, relative to what we would naturally expect; however, there were many findings entwine to previous knowledge about Diabetes. One of the more surprising findings that we saw had to deal with Heavy Alcohol Consumption (HvyAlcoholConsump). We noticed that of those who did not drink heavily, there was a “large” percentage of people with diabetes; whereas, those who did drink heavily, there was a small percentage of people with diabetes. These types of things are why EDA is so necessary when working with data. It can tell you things that you may not have expected previously, and can help strengthen your capabilities when undergoing analysis.\nThis concept is exactly what we will be going to do, as you can head to our Model Fitting site to see how we used our EDA to build several different models."
  },
  {
    "objectID": "Modeling.html#purp",
    "href": "Modeling.html#purp",
    "title": "Building Models",
    "section": "",
    "text": "The purpose of this document is to apply what we have learned in the exploratory data analysis page, and build some models to try and predict whether or not an individual will have diabetes. This document will make use of three different types of models: Logistic Regression, Classification Trees, and Random Forests. The descriptions of these models will be outlined in their respective sections. To compare these models, we will use log-loss as our criteria for defining the best model.\nWe are going to be using log-loss as our criteria, instead of accuracy, as mentioned in the above paragraph. Loss-loss criteria, popularly used in binary/classification analyses, is a special form of the likelihood function, and can be used to assess performance of classification models. Unlike certain criteria (i.e., accuracy), Log-loss penalizes models more heavily when incorrectly classifying observations. This is a main reason as to why we are choosing Log-loss over accuracy.\nNOw, let us start with our model-building process!"
  },
  {
    "objectID": "Modeling.html#model-1",
    "href": "Modeling.html#model-1",
    "title": "Building Models",
    "section": "Model 1",
    "text": "Model 1\nFor our first model, we are going to be using all 14 feature variables of interest that we identified in the EDA document.\n\nlogOne &lt;- train(Diabetes_binary ~ ., data = diabetesTrain, method = \"glm\", family = \"binomial\",\n                trControl = trctrl, tuneLength = 10, metric = \"logLoss\")"
  },
  {
    "objectID": "Modeling.html#model-2",
    "href": "Modeling.html#model-2",
    "title": "Building Models",
    "section": "Model 2",
    "text": "Model 2\nThe next model will only take the variables that had the biggest percentage of diabetes differences between levels, and will not include the BMI variable\n\nlogTwo &lt;- train(Diabetes_binary ~ HighBP + HighChol + CholCheck + Stroke + HeartDiseaseorAttack +\n                  HvyAlcoholConsump + DiffWalk, data = diabetesTrain, method = \"glm\", \n                family = \"binomial\", trControl = trctrl, tuneLength = 10, metric = \"logLoss\")"
  },
  {
    "objectID": "Modeling.html#model-3",
    "href": "Modeling.html#model-3",
    "title": "Building Models",
    "section": "Model 3",
    "text": "Model 3\nThe last model will only use the HighBP and HighChol variables, and will include an interaction between the two\n\nlogThree &lt;- train(Diabetes_binary ~ HighBP*HighChol, data = diabetesTrain, method = \"glm\",\n                  family = \"binomial\", trControl = trctrl, tuneLength = 10, metric = \"logLoss\")"
  },
  {
    "objectID": "Modeling.html#model-comparison",
    "href": "Modeling.html#model-comparison",
    "title": "Building Models",
    "section": "Model Comparison",
    "text": "Model Comparison\nAs we mentioned in the Purpose section, we will be using Log-Loss as our criteria for selecting the best model. Therefore, we will find the log loss value for each of the three models and see which model yields the lowest values. That model will be our “best” logistic regression model candidate\n\nlogLossOne &lt;- round(logOne$results$logLoss, 4)\nlogLossTwo &lt;- round(logTwo$results$logLoss, 4)\nlogLossThree &lt;- round(logThree$results$logLoss, 4)\n\npaste(\"Log Loss Criteria Values:\")\n\n[1] \"Log Loss Criteria Values:\"\n\ncat(\"\\n\")\npaste(\"Model 1:\", logLossOne)\n\n[1] \"Model 1: 0.3177\"\n\npaste(\"Model 2:\", logLossTwo)\n\n[1] \"Model 2: 0.3453\"\n\npaste(\"Model 3:\", logLossThree)\n\n[1] \"Model 3: 0.3597\"\n\n\nAs we can see from the above output, Model 1 - which contained all 14 feature variables - has the smallest Log-Loss value. This means that Model 1 is our “best” model, and we will use it to compare against the “best” classification tree model and random forest model."
  },
  {
    "objectID": "Modeling.html#model-1-1",
    "href": "Modeling.html#model-1-1",
    "title": "Building Models",
    "section": "Model 1",
    "text": "Model 1\nThe first classification tree model will be the same as the first logistic regression model. It will include all 14 feature variables.\n\nclassOne &lt;- train(Diabetes_binary ~ ., data = diabetesTrain, method = \"rpart\", metric = \"logLoss\",\n                  trControl = trctrl,\n                  tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001)))"
  },
  {
    "objectID": "Modeling.html#model-2-1",
    "href": "Modeling.html#model-2-1",
    "title": "Building Models",
    "section": "Model 2",
    "text": "Model 2\nThe next classification tree model will be the same as the second logistic regression model. It will include only the categorical variables that seemed to have the most impact on whether or not an individual has diabetes\n\nclassTwo &lt;- train(Diabetes_binary ~ HighBP + HighChol + CholCheck + Stroke + HeartDiseaseorAttack\n                  + HvyAlcoholConsump + DiffWalk, data = diabetesTrain, method = \"rpart\",\n                  metric = \"logLoss\", trControl = trctrl,\n                  tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001)))"
  },
  {
    "objectID": "Modeling.html#model-3-1",
    "href": "Modeling.html#model-3-1",
    "title": "Building Models",
    "section": "Model 3",
    "text": "Model 3\nThe last classification tree model will also be the same as the third logistic regression model. We will only use the HighBP and HighChol variables, and include an interaction between the two.\n\nclassThree &lt;- train(Diabetes_binary ~ HighBP*HighChol, data = diabetesTrain, method = \"rpart\",\n                    metric = \"logLoss\", trControl = trctrl,\n                    tuneGrid = expand.grid(cp = seq(from = 0, to = 0.1, by = 0.001)))"
  },
  {
    "objectID": "Modeling.html#model-comparison-1",
    "href": "Modeling.html#model-comparison-1",
    "title": "Building Models",
    "section": "Model Comparison",
    "text": "Model Comparison\nNow that we have built our three classification tree models, we can now begin to compare them. We will compare them, once again, based off the one with the best Log-Loss value.\n\noneMin &lt;- min(classOne$results$logLoss)\ntwoMin &lt;- min(classTwo$results$logLoss)\nthreeMin &lt;- min(classThree$results$logLoss)\nfor (i in 1:101){\n  if (classOne$results$logLoss[i] == oneMin) {\n    oneCP = classOne$results$cp[i]\n  }\n  if (classTwo$results$logLoss[i] == twoMin) {\n    twoCP = classTwo$results$cp[i]\n  }\n  if (classThree$results$logLoss[i] == threeMin) {\n    threeCP = classThree$results$cp[i]\n  }\n}\n\npaste(\"Log Loss Criteria Values (with cp): \")\n\n[1] \"Log Loss Criteria Values (with cp): \"\n\ncat(\"\\n\")\npaste0(\"Model 1: \", round(oneMin, 4), \" (cp = \", oneCP, \")\")\n\n[1] \"Model 1: 0.3565 (cp = 0.001)\"\n\npaste0(\"Model 2: \", round(twoMin, 4), \" (cp = \", twoCP, \")\")\n\n[1] \"Model 2: 0.3599 (cp = 0)\"\n\npaste0(\"Model 3: \", round(threeMin, 4), \" (cp = \", threeCP, \")\")\n\n[1] \"Model 3: 0.4032 (cp = 0.1)\"\n\n\nAs we can see, model 1 is once again the “best” model for the classification tree models. It yields the lowest log-loss value, so we will use that model when doing our final comparisons."
  }
]